import mathimport subprocess as spimport psutilimport torchdef calc_cuda(n_users):    "how many users can the GPU support"        command = "nvidia-smi --query-gpu=memory.free --format=csv"    memory_free_info = sp.check_output(command.split()).decode('ascii').split('\n')[:-1][1:]    memory_free_value = [int(x.split()[0]) for i, x in enumerate(memory_free_info)][0]        command = "nvidia-smi --query-gpu=memory.total --format=csv"    memory_total_info = sp.check_output(command.split()).decode('ascii').split('\n')[:-1][1:]    memory_total_value = [int(x.split()[0]) for i, x in enumerate(memory_total_info)][0]                              memory_used_value = memory_total_value - memory_free_value        return math.floor(memory_total_value / (memory_used_value / n_users))def calc_mps(n_users):    "how many users can the MPS backend support"        return math.floor(psutil.virtual_memory()[0] * 1.15 / (torch.mps.driver_allocated_memory() / n_users))def calc_max_users(n_users):    "given current existing user(s), how many can the machine support"        if torch.cuda.is_available():        device = "cuda"    elif torch.backends.mps.is_available() and torch.backends.mps.is_built():        device = "mps"    else:        device = "cpu"        if device == "cpu":        max_users = 1    elif device == "mps":        max_users = calc_mps(n_users)    elif device == "cuda":        max_users =  calc_cuda(n_users)        return max_users