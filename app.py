import streamlit as stimport hmacimport pandas as pdimport sysimport gcfrom datetime import datetime, timedeltaimport timeimport osfrom streamlit_server_state import server_state, server_state_lockfrom helper.modelling import initializefrom helper.own_corpus import process_corpus### session initialization/login# last used timeif os.path.exists("metadata/last_used.txt") and "last_used" not in st.session_state:    file = open("metadata/last_used.txt", "r")    st.session_state["last_used"] = datetime.strptime(file.read(), '%Y-%m-%d %H:%M:%S.%f')    file.close()else:    st.session_state["last_used"] = datetime.now() - timedelta(hours=0, minutes=10)# last userif os.path.exists("metadata/user.txt"):    file = open("metadata/user.txt", "r")    st.session_state["last_user"] = file.read()    file.close()else:    st.session_state["last_user"] = "none"if "users_list" not in st.session_state:    st.session_state["users_list"] = pd.read_csv("metadata/user_list.csv")# passworddef check_password():    """Returns `True` if the user had the correct password."""    st.session_state["available"] = (datetime.now() - st.session_state["last_used"]).total_seconds() > 1# available if last use more than 3 minutes ago            if not(st.session_state["available"]):        st.error(f"""Application in use by [{st.session_state['last_user']}](mailto:{st.session_state["users_list"].loc[lambda x: x.user == st.session_state['last_user'], 'email'].values[0]}). Refresh in 3 minutes, if they have stopped using it you will be able to log in.""")    def password_entered():        """Checks whether a password entered by the user is correct."""        if hmac.compare_digest(st.session_state["password"], st.secrets["password"]):            st.session_state["password_correct"] = True            del st.session_state["password"]  # Don't store the password.        else:            st.session_state["password_correct"] = False    # Return True if the password is validated.    if st.session_state.get("password_correct", False):        if st.session_state["available"]:            return True        # show input for user name    st.session_state["user_name"] = st.selectbox(       "User",       st.session_state["users_list"],       index=None,       placeholder="Select user...",    )    # Show input for password.    st.text_input(        "Password", type="password", on_change=password_entered, key="password"    )    if "password_correct" in st.session_state:        st.error("Password incorrect")    return Falseif not check_password():    st.stop()  # Do not continue if check_password is not True.    ###  app setup# headerst.title("UNCTAD LLM")# Ddo not continue if a new user has booted off this oneif st.session_state["user_name"] != st.session_state["last_user"] and "user_recorded" in st.session_state:    if "model" in st.session_state:        st.session_state["model"].close_connection()        del st.session_state["model"].llm        del st.session_state["model"]        gc.collect()    st.error(f"""[{st.session_state['last_user']}](mailto:{st.session_state["users_list"].loc[lambda x: x.user == st.session_state['last_user'], 'email'].values[0]}) has logged on. Refresh in 3 minutes, if they have stopped using it you will be able to log in.""")    st.stop()    # record the userif "user_recorded" not in st.session_state:    f = open("metadata/user.txt", "w")    f.write(st.session_state["user_name"])    f.close()    st.session_state["user_recorded"] = True    server_state["user_name"] = st.session_state["user_name"]print(f'New user: {server_state["user_name"]}')    # record last interactionf = open("metadata/last_used.txt", "w")f.write(str(datetime.now()))f.close()# styles sheetswith open( "styles/style.css" ) as css:    st.markdown( f'<style>{css.read()}</style>' , unsafe_allow_html= True)    user_avatar = "https://www.svgrepo.com/show/524211/user.svg"#"\N{grinning face}"assistant_avatar = "https://www.svgrepo.com/show/375527/ai-platform.svg"#"\N{Robot Face}"# LLM set up# parameters/authenticationllm_dict = pd.read_csv("metadata/llm_list.csv")corpora_dict = pd.read_csv("metadata/corpora_list.csv")db_info = pd.read_csv("metadata/db_creds.csv")# placeholder on initial loadif "model" not in st.session_state:    st.markdown("""<div class="icon_text"><img width=50 src='https://www.svgrepo.com/show/375527/ai-platform.svg'></div>""", unsafe_allow_html=True)    st.markdown("""<div class="icon_text"<h4>What would you like to know?</h4></div>""", unsafe_allow_html=True)### sidebar# upload your own documents    st.sidebar.markdown("# Upload your own documents", help = "Enter the name of your corpus in the `Corpus name` field. If named `temporary`, it will be able to be written over after your session.")# paste a list of web urlsst.session_state["own_urls"] = st.sidebar.text_input(   "URLs",   value="" if "own_urls" not in st.session_state else st.session_state["own_urls"],   help="A comma separated list of URLs.")st.session_state["uploaded_file"] = st.sidebar.file_uploader("Upload your own documents",type=[".zip", ".docx", ".doc", ".txt", ".pdf", ".csv"], help="Upload either a single `metadata.csv` file, with at least one column named `web_filepath` with the web addresses of the .html or .pdf documents, or upload a .zip file that contains a folder named `corpus` with the .doc, .docx, .txt, or .pdf files inside. You can optionally include a `metadata.csv` file in the zip file at the same level as the `corpus` folder, with at least a column named `filename` with the names of the files.")st.session_state["new_corpus_name"] = st.sidebar.text_input(   "Uploaded corpus name",   value="temporary" if "new_corpus_name" not in st.session_state else st.session_state["new_corpus_name"],   help="The name of the new corpus. It must be able to be a SQL database name, so only lower case, no special characters, no spaces. Use underscores.")with st.sidebar.form("corpus_buttons"):    process_corpus_button = st.form_submit_button('Process corpus')    reset_memory = st.form_submit_button("Reset model's memory")# model paramsst.sidebar.markdown("# Model parameters", help="Click the `Reinitialize model` button if you change any of these parameters.")# which_llmst.session_state["selected_llm"] = st.sidebar.selectbox(   "Which LLM",   options=llm_dict.name,   index=tuple(llm_dict.name).index("mistral-docsgpt") if "selected_llm" not in st.session_state else tuple(llm_dict.name).index(st.session_state["selected_llm"]),   help="Which LLM to use.")# which corpusst.session_state["selected_corpus"] = st.sidebar.selectbox(   "Which corpus",   options=["None"] + list(corpora_dict.name),   index=0 if "selected_corpus" not in st.session_state else tuple(["None"] + list(corpora_dict.name)).index(st.session_state["selected_corpus"]),   help="Which corpus to contextualize on.")# similarity top kst.session_state["similarity_top_k"] = st.sidebar.slider(   "Similarity top K",   min_value=1,   max_value=20,   step=1,   value=4 if "similarity_top_k" not in st.session_state else st.session_state["similarity_top_k"],   help="The number of contextual document chunks to retrieve for RAG.")# n_gpu layersst.session_state["n_gpu_layers"] = 100 if "n_gpu_layers" not in st.session_state else st.session_state["n_gpu_layers"]# temperaturest.session_state["temperature"] = st.sidebar.slider(   "Temperature",   min_value=0,   max_value=100,   step=1,   value=0 if "temperature" not in st.session_state else st.session_state["temperature"],   help="How much leeway/creativity to give the model, 0 = least creativity, 100 = most creativity.")# max_new tokensst.session_state["max_new_tokens"] = st.sidebar.slider(   "Max new tokens",   min_value=16,   max_value=16000,   step=8,   value=512 if "max_new_tokens" not in st.session_state else st.session_state["max_new_tokens"],   help="How long to limit the responses to (token â‰ˆ word).")# context windowst.session_state["context_window"] = st.sidebar.slider(   "Context window",   min_value=500,   max_value=50000,   step=100,   value=4000 if "context_window" not in st.session_state else st.session_state["context_window"],   help="How large to make the context window for the LLM. The maximum depends on the model, a higher value might result in context window too large errors.")# memory limitst.session_state["memory_limit"] = st.sidebar.slider(   "Memory limit",   min_value=80,   max_value=80000,   step=8,   value=2048 if "memory_limit" not in st.session_state else st.session_state["memory_limit"],   help="How many tokens (words) memory to give the chatbot.")# system promptst.session_state["system_prompt"] = st.sidebar.text_input(   "System prompt",   value=""  if "system_prompt" not in st.session_state else st.session_state["system_prompt"],   help="What prompt to initialize the chatbot with.")# params that affect the vector_dbst.sidebar.markdown("# Vector DB parameters", help="Changing these parameters will require remaking the vector database and require a bit longer to run. Push the `Reinitialize model and remake DB` button if you change one of these.")# chunk overlapst.session_state["chunk_overlap"] = st.sidebar.slider(   "Chunk overlap",   min_value=0,   max_value=1000,   step=1,   value=200 if "chunk_overlap" not in st.session_state else st.session_state["chunk_overlap"],   help="How many tokens to overlap when chunking the documents.")# chunk sizest.session_state["chunk_size"] = st.sidebar.slider(   "Chunk size",   min_value=64,   max_value=6400,   step=8,   value=512 if "chunk_size" not in st.session_state else st.session_state["chunk_size"],   help="How many tokens per chunk when chunking the documents.")# reinitialize model buttonwith st.sidebar.form("model_buttons"):    reinitialize = st.form_submit_button('Reinitialize model')    reinitialize_remake = st.form_submit_button('Reinitialize model and remake DB')    # help contactst.sidebar.markdown("*For questions on how to use this application or its methodology, please write [Daniel Hopp](mailto:daniel.hopp@un.org)*", unsafe_allow_html=True)    # static model paramsparagraph_separator = "\n\n\n"separator = " "use_chat_engine = Truereset_chat_engine = Falsedb_name = "vector_db"if "rerun_populate_db" not in st.session_state:    st.session_state["rerun_populate_db"] = Falseif "clear_database" not in st.session_state:    st.session_state["clear_database"] = Falseif "model" not in st.session_state or reinitialize or reinitialize_remake or process_corpus_button:    with st.spinner('Processing corpus...'):        if process_corpus_button:            if not((st.session_state["new_corpus_name"] == "temporary") or (st.session_state["new_corpus_name"] not in list(corpora_dict.name.values))):                st.error("A corpus with this name already exists, choose another one.")            else:                process_corpus(corpus_name=st.session_state["new_corpus_name"], own_urls=st.session_state["own_urls"], uploaded_document=st.session_state["uploaded_file"])                st.session_state["selected_corpus"] = st.session_state["new_corpus_name"]                st.session_state.messages = [] # clear out message history on the prior context                st.info("New corpus successfully processed!")            if "model" in st.session_state:        print(st.session_state["selected_corpus"])        if st.session_state["which_corpus"] is not None:            st.session_state["model"].close_connection()        del st.session_state["model"].llm        del st.session_state["model"]        gc.collect()        with st.spinner('Initializing...'):        # whether or not to remake the vector DB        if reinitialize_remake or process_corpus_button:            rerun_populate_db = True            clear_database_local = True        else:            rerun_populate_db = st.session_state["rerun_populate_db"]            clear_database_local = st.session_state["clear_database"]                st.session_state["model"], st.session_state["which_llm"], st.session_state["which_corpus"] = initialize(            which_llm_local=st.session_state["selected_llm"],            which_corpus_local=None if st.session_state["selected_corpus"] == "None" else st.session_state["selected_corpus"],            n_gpu_layers=st.session_state["n_gpu_layers"],            temperature=st.session_state["temperature"] / 1e2, # convert 1-100 to 0-1            max_new_tokens=st.session_state["max_new_tokens"],            context_window=st.session_state["context_window"],            memory_limit=st.session_state["memory_limit"],            chunk_overlap=st.session_state["chunk_overlap"],            chunk_size=st.session_state["chunk_size"],            paragraph_separator=paragraph_separator,            separator=separator,            system_prompt=st.session_state["system_prompt"],            rerun_populate_db=rerun_populate_db,            clear_database_local=clear_database_local,            corpora_dict=corpora_dict,            llm_dict=llm_dict,            db_name=db_name,            db_info=db_info,        )                st.session_state.messages = [] # clear out message history on the prior context        st.info("Model successfully initialized!")# Initialize chat historyif "messages" not in st.session_state:    st.session_state.messages = []# Display chat messages from history on app rerunfor message in st.session_state.messages:    avatar = user_avatar if message["role"] == "user" else assistant_avatar    with st.chat_message(message["role"], avatar=avatar):        if "source_string" not in message["content"]:            st.markdown(message["content"])        else:            st.markdown("Sources: ", unsafe_allow_html=True, help=message["content"].split("string:")[1])            # reset model's memoryif reset_memory:    if st.session_state["model"].chat_engine is not None:        st.session_state["model"].chat_engine.reset()    with st.chat_message("assistant", avatar=assistant_avatar):        st.markdown("Model memory reset!")    st.session_state.messages.append({"role": "assistant", "content": "Model memory reset!"})# Accept user inputif prompt := st.chat_input('Query the LLM...'):    # Display user message in chat message container    with st.chat_message("user", avatar=user_avatar):        st.markdown(prompt)    # Add user message to chat history    st.session_state.messages.append({"role": "user", "content": prompt})        # record last interaction    f = open("metadata/last_used.txt", "w")    f.write(str(datetime.now()))    f.close()        if st.session_state.messages[-1]["content"].lower() == "clear":        if st.session_state["which_corpus"] is not None:            st.session_state["model"].close_connection()        del st.session_state["model"].llm        del st.session_state["model"]        gc.collect()        with st.chat_message("assistant", avatar=assistant_avatar):            st.markdown("Models cleared!")        st.session_state.messages.append({"role": "assistant", "content": "Models cleared!"})    else:        response = st.session_state["model"].gen_response(            st.session_state.messages[-1]["content"],            similarity_top_k=st.session_state["similarity_top_k"],            use_chat_engine=use_chat_engine,            reset_chat_engine=reset_chat_engine,            streaming=True,        )        def streamed_response(streamer):            with st.spinner('Thinking...'):                for token in streamer.response_gen:                    yield token            # Display assistant response in chat message container        with st.chat_message("assistant", avatar=assistant_avatar):            st.write_stream(streamed_response(response["response"]))            # adding sources        with st.chat_message("assistant", avatar=assistant_avatar):            if len(response.keys()) > 1: # only do if RAG                # markdown help way                source_string = ""                counter = 1                for j in list(pd.Series(list(response.keys()))[pd.Series(list(response.keys())) != "response"]):                    #source_string += f"**Source {counter}**:\n\n \t\t{response[j]}\n\n\n\n"                    metadata_dict = eval(response[j].split("| source text:")[0].replace("metadata: ", ""))                    metadata_string = ""                    for key, value in metadata_dict.items():                        metadata_string += f"'{key}': '{value}'\n"                                        source_string += f"""# Source {counter}\n ### Metadata:\n ```{metadata_string}```\n ### Text:\n{response[j].split("| source text:")[1]}\n\n"""                    counter += 1            else:                source_string = "NA"            st.markdown("Sources: ", unsafe_allow_html=True, help = f"{source_string}")            # Add assistant response to chat history        st.session_state.messages.append({"role": "assistant", "content": response["response"].response})        st.session_state.messages.append({"role": "assistant", "content": f"source_string:{source_string}"})