import timeimport pandas as pdimport streamlit as stfrom streamlit_server_state import no_rerun, server_statefrom helper.modelling import determine_rerun_reinitialize, load_model, set_static_model_paramsfrom helper.ui import import_styles, streamed_response, ui_advanced_model_params, ui_export_chat_end_session, ui_lockout_reset, ui_model_params, ui_upload_docsfrom helper.user_management import check_password, clear_models, determine_availability, manage_boot, setup_local_files, update_server_state### session initialization/loginif "user_name" in st.session_state:    from helper.user_management import record_usedetermine_availability()if not check_password():    st.stop()  # Do not continue if check_password is not True.        ### initial setup# headerst.title("Local LLM")if "master_db_name" not in st.session_state:    st.session_state["master_db_name"] = "vector_db"if "db_name" not in st.session_state:    st.session_state["db_name"] = st.session_state["user_name"].lower().replace(" ", "_")    # check if first boot of user and check if eligible to be kicked offmanage_boot()        # styles sheetsimport_styles()# parameters/authenticationsetup_local_files()# placeholder on initial loadif f'model_{st.session_state["db_name"]}' not in server_state:    st.markdown("""<div class="icon_text"><img width=50 src='https://www.svgrepo.com/show/375527/ai-platform.svg'></div>""", unsafe_allow_html=True)    st.markdown("""<div class="icon_text"<h4>What would you like to know?</h4></div>""", unsafe_allow_html=True)### sidebar# upload your own documents sectionui_upload_docs()st.sidebar.divider()# model parametersui_model_params()# advanced model parametersui_advanced_model_params()st.sidebar.divider()# lockout and reset memory buttonui_lockout_reset()### model    # static model paramsset_static_model_params()# determine if the database needs to be reinitializeddetermine_rerun_reinitialize()# loading modelload_model()if f'model_{st.session_state["db_name"]}' in server_state:    # Initialize chat history    if "messages" not in st.session_state:        st.session_state.messages = []        # Display chat messages from history on app rerun    for message in st.session_state.messages:        avatar = st.session_state["user_avatar"] if message["role"] == "user" else st.session_state["assistant_avatar"]        with st.chat_message(message["role"], avatar=avatar):            if "source_string" not in message["content"]:                st.markdown(message["content"])            else:                st.markdown("Sources: ", unsafe_allow_html=True, help=message["content"].split("string:")[1])                    # reset model's memory    if st.session_state["reset_memory"]:        if server_state[f'model_{st.session_state["db_name"]}'].chat_engine is not None:            with no_rerun:                server_state[f'model_{st.session_state["db_name"]}'].chat_engine.reset()        with st.chat_message("assistant", avatar=st.session_state["assistant_avatar"]):            st.markdown("Model memory reset!")        st.session_state.messages.append({"role": "assistant", "content": "Model memory reset!"})        # Accept user input    if st.session_state["which_corpus"] is None:        placeholder_text = f"""Query '{st.session_state["which_llm"]}', not contextualized"""    else:        placeholder_text = f"""Query '{st.session_state["which_llm"]}' contextualized on '{st.session_state["which_corpus"]}' corpus"""            if prompt := st.chat_input(placeholder_text):        # Display user message in chat message container        with st.chat_message("user", avatar=st.session_state["user_avatar"]):            st.markdown(prompt)        # Add user message to chat history        st.session_state.messages.append({"role": "user", "content": prompt})                if st.session_state.messages[-1]["content"].lower() == "clear":            clear_models()                with st.chat_message("assistant", avatar=st.session_state["assistant_avatar"]):                st.markdown("Models cleared!")            st.session_state.messages.append({"role": "assistant", "content": "Models cleared!"})        else:            # lock the model to perform requests sequentially            if "in_use" not in server_state:                update_server_state("in_use", False)                            if "exec_queue" not in server_state:                update_server_state("exec_queue", [st.session_state["user_name"]])            if len(server_state["exec_queue"]) == 0:                update_server_state("exec_queue", [st.session_state["user_name"]])            else:                if st.session_state["user_name"] not in server_state["exec_queue"]:                    # add to the queue                    update_server_state("exec_queue", server_state["exec_queue"] + [st.session_state["user_name"]])                            with st.spinner('Query queued...'):                t = st.empty()                while server_state["in_use"] or server_state["exec_queue"][0] != st.session_state["user_name"]:                    t.markdown(f'You are place {server_state["exec_queue"].index(st.session_state["user_name"])} of {len(server_state["exec_queue"]) - 1}')                    time.sleep(1)                t.empty()                                # lock the model while generating            update_server_state("in_use", True)            record_use(future_lock=True)                            # generate response                            response = server_state[f'model_{st.session_state["db_name"]}'].gen_response(                st.session_state.messages[-1]["content"],                similarity_top_k=st.session_state["similarity_top_k"],                use_chat_engine=st.session_state["use_chat_engine"],                reset_chat_engine=st.session_state["reset_chat_engine"],                streaming=True,            )            # Display assistant response in chat message container            with st.chat_message("assistant", avatar=st.session_state["assistant_avatar"]):                st.write_stream(streamed_response(response["response"]))                    # adding sources            with st.chat_message("assistant", avatar=st.session_state["assistant_avatar"]):                if len(response.keys()) > 1: # only do if RAG                    # markdown help way                    source_string = ""                    counter = 1                    for j in list(pd.Series(list(response.keys()))[pd.Series(list(response.keys())) != "response"]):                        #source_string += f"**Source {counter}**:\n\n \t\t{response[j]}\n\n\n\n"                        metadata_dict = eval(response[j].split("| source text:")[0].replace("metadata: ", ""))                        metadata_string = ""                        for key, value in metadata_dict.items():                            if key != "is_csv":                                metadata_string += f"'{key}': '{value}'\n"                                                source_string += f"""# Source {counter}\n ### Metadata:\n ```{metadata_string}```\n ### Text:\n{response[j].split("| source text:")[1]}\n\n"""                        counter += 1                else:                    source_string = "NA"                st.markdown("Sources: ", unsafe_allow_html=True, help = f"{source_string}")                            # unlock the model            update_server_state("in_use", False)            update_server_state("exec_queue", server_state["exec_queue"][1:]) # take out of the queue                        record_use(future_lock=False)                    # Add assistant response to chat history            st.session_state.messages.append({"role": "assistant", "content": response["response"].response})            st.session_state.messages.append({"role": "assistant", "content": f"source_string:{source_string}"})            ### final UI elementsui_export_chat_end_session()